---
title: "Model biulding for predicting barbell lift manners"
author: "Xiaozhuo Wang"
date: "2017-09-18"
output: html_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```

Background
----------

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants, who were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal of our project is to predict the manner in which they did the exercise. More information and the data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

Data preparation
----------------

The training data for this project are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv), 
the test data are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).

```{r}
## Loading data and packages
if(!file.exists("pml-training.csv")) {
        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
              "pml-training.csv")}
if(!file.exists("pml-training.csv")) {
        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
              "pml-testing.csv")}
pml_training <- read.csv("pml-training.csv")
pml_testing <- read.csv("pml-testing.csv")
library(caret)
```

When exploring the data with `summary` function, it can be found that there are plenty 
of colums filled with NAs or blanks, such variables are excluded. Some colums like 
row numbers, user names and time stamps are also excluded considering the goal of 
this project. The `classe` variable (the manner in which they did the exercise) is 
the outcome, and the remained 52 variables will be used as predictors. 

The `pml_training` data are sliced into 70% for training and 30% for validing.

```{r, cache=TRUE}
## Data preprocessing
suma <- summary(pml_training)
isna <- NULL; for (i in 1:160) {isna[i] = sum(is.na(pml_training[,i])) == 19216}
isbl <- grepl(":19216",suma[1,])
iskept <- !(isna|isbl)
iskept[1:7] <- FALSE
pre_training <- pml_training[,iskept]
testing <- pml_testing[,iskept]

set.seed(2333)
inTrain <- createDataPartition(y = pre_training$classe, p = 0.7, list = FALSE)
training <- pre_training[inTrain,]
validing <- pre_training[-inTrain,]
dims <- data.frame(rbind(dim(training), dim(validing), dim(testing)))
colnames(dims) <- c("Observation","Variable")
rownames(dims) <- c("Training","Validing","Testing")
dims
```

Model biulding
--------------

In this project, three training methods are performed and compared to choose the 
best one: random forest (rf), boosting with trees (gbm) and quadratic discriminant 
analysis (qda). `rf` and `gbm` are chosen because of their high accuracy, `qda` is chosen 
because of reduced computational complexity (assumes multivariate Gaussian distribution 
with different covariance for predictor variables).

The `rf` model used train control with a 10-fold cross validation method to avoid 
over-fitting. Other parameters in model training are all by default of `train` function 
of `caret` package.

The time consuming for training models is listed below. `rf` model has the highest 
computational complexity, costs over 200 times and twice the training time for `qda` 
and `gbm` medels.

```{r, cache=TRUE}
## Model training
fitControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
set.seed(2333); t1 <- Sys.time()
mod_rf <- train(classe ~ ., data = training, method = "rf", 
                trControl = fitControl); t2 <- Sys.time()
mod_gbm <- train(classe ~ ., data = training, method = "gbm", verbose = FALSE, 
                 trControl = fitControl); t3 <- Sys.time()
mod_qda <- train(classe ~ ., data = training, method = "qda", 
                 trControl = fitControl); t4 <- Sys.time()

time_rf <- as.double(t2 - t1, units = "secs")
time_gbm <- as.double(t3 - t2, units = "secs")
time_qda <- as.double(t4 - t3, units = "secs")
time <- data.frame(round(rbind(rf = time_rf, gbm = time_gbm, qda = time_qda), 0))
time <- cbind(time,c("s","s","s"))
colnames(time) <- c("Time for training","")
time
```

Model validing
--------------

```{r, cache=TRUE}
## Model validing
insam_rf<- confusionMatrix(training$classe, predict(mod_rf, training))$overall
insam_gbm<- confusionMatrix(training$classe, predict(mod_gbm, training))$overall
insam_qda<- confusionMatrix(training$classe, predict(mod_qda, training))$overall
otsam_rf<- confusionMatrix(validing$classe, predict(mod_rf, validing))$overall
otsam_gbm<- confusionMatrix(validing$classe, predict(mod_gbm, validing))$overall
otsam_qda<- confusionMatrix(validing$classe, predict(mod_qda, validing))$overall
accuracy <- rbind(insam_rf, otsam_rf, insam_gbm, otsam_gbm, insam_qda, otsam_qda)
row.names(accuracy) <- c("mod_rf :in sample","        out of sample",
                         "mod_gbm:in sample","        out of sample",
                         "mod_qda:in sample","        out of sample")
accuracy <- round(accuracy[,c(1,3,4,2)],3)
accuracy
```

The accuracy of predicted validing data are used for estimating out of sample error. 
As the above table shows, the `rf` model has the overall highest accuracy (both in 
sample and out of sample) among the three models. It can be seen that out of sample 
error is higher than in sample error of rf model, but still higher than 0.99. The 
`gbm` model has the medium accuracy and the `qda` model has the lowest accuracy.

The distribution plot of possible number of right answers predicted by the three 
models are created below:

```{r}
x <- rep(0:20,each=3)
mod <- rep(c("qda","gbm","rf"),21)
dens <- NULL
for (i in 0:20) {dens <- c(dens,
                           dbinom(i, size = 20, prob = otsam_qda[1]), 
                           dbinom(i, size = 20, prob = otsam_gbm[1]), 
                           dbinom(i, size = 20, prob = otsam_rf[1]))}
dens <- data.frame(x, mod, dens)
fig <- ggplot(dens, aes(x = x, y = dens, col = mod)) + 
        geom_point(size = 2) + 
        geom_line(size = 1) + 
        coord_cartesian(xlim = c(15,20), ylim = c(0,1)) + 
        labs(x = "Number of right predictions",
             y = "Probability")
fig
p <- round(dbinom(20, size = 20, prob = otsam_rf[1]),3)
```

It's easy to find that the `rf` model has the highest probability of getting all-right 
prediction (`r p`). Thus we chose it as our final model for predicting 
the testing data.

Predicting results
------------------

```{r, cache=TRUE}
right_answer <- factor(c("B","A","B","A","A","E","D","B","A","A",
                         "B","C","B","A","E","E","A","B","B","B"))
ra_rf <- sum(predict(mod_rf, testing) == right_answer)
ra_gbm <- sum(predict(mod_gbm, testing) == right_answer)
ra_qda <- sum(predict(mod_qda, testing) == right_answer)
accu_rf <- ra_rf / 20
accu_gbm <- ra_gbm / 20
accu_qda <- ra_qda / 20
```

The predictions of the `rf` model get `r ra_rf` right out of 20 observations in 
testing data, which means an excellent `r accu_rf*100`% accuracy. 

The accuracy of `gbm` and `qda` models are also tested, in this case, they are 
`r ra_gbm`/20 and `r ra_qda`/20, respectively.

*The source code for this project can be reviewed [here](https://github.com/catree1988/practicalmachinelearning).*
